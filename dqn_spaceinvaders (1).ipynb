{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn_spaceinvaders.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " C처digo foi feito a partir de um tutorial, ent찾o n찾o foi implementado do zero mas foi completo por n처s\n",
        " \n",
        " https://www.coursera.org/learn/practical-rl"
      ],
      "metadata": {
        "id": "kZ2dzuM4hKTP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seT-YHg5vziF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2b53bd-445e-468f-af4b-1e167c5ca0b3"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "    %tensorflow_version 1.x\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "bash: ../xvfb: No such file or directory\n",
            "env: DISPLAY=:1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "65Brh27XhHqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C5cmoBmvziM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f0542f-1a9a-4fbe-d498-7e2b73a91d03"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unrar\n",
            "  Downloading unrar-0.4-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Roms.rar\n",
            "\n",
            "Extracting  HC ROMS.zip                                                  \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  ROMS.zip                                                     \b\b\b\b 74%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n",
            "copying adventure.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Adventure (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Air Raid (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Alien.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying crazy_climber.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Crazy Climber.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying elevator_action.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Elevator Action (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying gravitar.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Gravitar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying keystone_kapers.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Keystone Kapers (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from HC ROMS/BY ALPHABET (PAL)/H-R/King Kong (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying laser_gates.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Laser Gates (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying mr_do.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Mr. Do! (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying pacman.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Pac-Man (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying jamesbond.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/James Bond 007.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying koolaid.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Kool-Aid Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Krull.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying montezuma_revenge.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Montezuma's Revenge - Featuring Panama Joe.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying star_gunner.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Stargunner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying time_pilot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Time Pilot.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying up_n_down.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Up 'n Down.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying sir_lancelot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/Sir Lancelot (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying amidar.bin from HC ROMS/BY ALPHABET/A-G/Amidar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying asteroids.bin from HC ROMS/BY ALPHABET/A-G/Asteroids [no copyright].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from HC ROMS/BY ALPHABET/A-G/Atlantis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from HC ROMS/BY ALPHABET/A-G/Bank Heist.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from HC ROMS/BY ALPHABET/A-G/Battlezone.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from HC ROMS/BY ALPHABET/A-G/Beamrider.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from HC ROMS/BY ALPHABET/A-G/Berzerk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from HC ROMS/BY ALPHABET/A-G/Bowling.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from HC ROMS/BY ALPHABET/A-G/Boxing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from HC ROMS/BY ALPHABET/A-G/Breakout - Breakaway IV.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from HC ROMS/BY ALPHABET/A-G/Carnival.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from HC ROMS/BY ALPHABET/A-G/Centipede.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from HC ROMS/BY ALPHABET/A-G/Chopper Command.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying defender.bin from HC ROMS/BY ALPHABET/A-G/Defender.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from HC ROMS/BY ALPHABET/A-G/Demon Attack.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from HC ROMS/BY ALPHABET/A-G/Donkey Kong.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from HC ROMS/BY ALPHABET/A-G/Double Dunk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying enduro.bin from HC ROMS/BY ALPHABET/A-G/Enduro.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from HC ROMS/BY ALPHABET/A-G/Fishing Derby.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from HC ROMS/BY ALPHABET/A-G/Freeway.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from HC ROMS/BY ALPHABET/A-G/Frogger.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from HC ROMS/BY ALPHABET/A-G/Frostbite.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from HC ROMS/BY ALPHABET/A-G/Galaxian.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from HC ROMS/BY ALPHABET/A-G/Gopher.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying hero.bin from HC ROMS/BY ALPHABET/H-R/H.E.R.O..bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from HC ROMS/BY ALPHABET/H-R/Ice Hockey.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying journey_escape.bin from HC ROMS/BY ALPHABET/H-R/Journey Escape.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from HC ROMS/BY ALPHABET/H-R/Kaboom!.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from HC ROMS/BY ALPHABET/H-R/Kangaroo.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying kung_fu_master.bin from HC ROMS/BY ALPHABET/H-R/Kung-Fu Master.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying lost_luggage.bin from HC ROMS/BY ALPHABET/H-R/Lost Luggage [no opening scene].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying ms_pacman.bin from HC ROMS/BY ALPHABET/H-R/Ms. Pac-Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from HC ROMS/BY ALPHABET/H-R/Name This Game.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying phoenix.bin from HC ROMS/BY ALPHABET/H-R/Phoenix.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying pitfall.bin from HC ROMS/BY ALPHABET/H-R/Pitfall! - Pitfall Harry's Jungle Adventure.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from HC ROMS/BY ALPHABET/H-R/Pooyan.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from HC ROMS/BY ALPHABET/H-R/Private Eye.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from HC ROMS/BY ALPHABET/H-R/Q-bert.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from HC ROMS/BY ALPHABET/H-R/River Raid.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of HC ROMS/BY ALPHABET/H-R/Road Runner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from HC ROMS/BY ALPHABET/H-R/Robot Tank.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from HC ROMS/BY ALPHABET/S-Z/Seaquest.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying skiing.bin from HC ROMS/BY ALPHABET/S-Z/Skiing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from HC ROMS/BY ALPHABET/S-Z/Solaris.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from HC ROMS/BY ALPHABET/S-Z/Space Invaders.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying surround.bin from HC ROMS/BY ALPHABET/S-Z/Surround - Chase.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from HC ROMS/BY ALPHABET/S-Z/Tennis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying trondead.bin from HC ROMS/BY ALPHABET/S-Z/TRON - Deadly Discs.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from HC ROMS/BY ALPHABET/S-Z/Tutankham.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying venture.bin from HC ROMS/BY ALPHABET/S-Z/Venture.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from HC ROMS/BY ALPHABET/S-Z/Video Olympics - Pong Sports.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying video_pinball.bin from HC ROMS/BY ALPHABET/S-Z/Video Pinball - Arcade Pinball.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying wizard_of_wor.bin from HC ROMS/BY ALPHABET/S-Z/Wizard of Wor.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from HC ROMS/BY ALPHABET/S-Z/Yars' Revenge.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from HC ROMS/BY ALPHABET/S-Z/Zaxxon.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "copying assault.bin from HC ROMS/NTSC VERSIONS OF PAL ORIGINALS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAfMgPNBvziQ"
      },
      "source": [
        "### Processing game image \n",
        "\n",
        "* converter de 210x160 pra 64x64\n",
        "* converter pro greyscale\n",
        "* Cortar Bordas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzuRB5OEvziV"
      },
      "source": [
        "from gym.core import ObservationWrapper\n",
        "from gym.spaces import Box\n",
        "import cv2\n",
        "\n",
        "class PreprocessAtari(ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        ObservationWrapper.__init__(self,env)\n",
        "        \n",
        "        self.img_size = (84, 84)\n",
        "        self.observation_space = Box(0.0, 1.0, (self.img_size[0], self.img_size[1], 1))\n",
        "\n",
        "    def observation(self, img):\n",
        "        \n",
        "        img = img[34:-16, :, :]\n",
        "        \n",
        "        \n",
        "        img = cv2.resize(img, self.img_size)\n",
        "        \n",
        "        img = img.mean(-1,keepdims=True)\n",
        "        \n",
        "        img = img.astype('float32') / 255.\n",
        "              \n",
        "        return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsb99gIIvzid"
      },
      "source": [
        "### Frame buffer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IA-czvUwbOn"
      },
      "source": [
        "from gym.core import Wrapper\n",
        "class FrameBuffer(Wrapper):\n",
        "    def __init__(self, env, n_frames=4):\n",
        "        super(FrameBuffer, self).__init__(env)\n",
        "        height, width, n_channels = env.observation_space.shape\n",
        "        obs_shape = [height, width, n_channels * n_frames] \n",
        "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
        "        \n",
        "    def reset(self):\n",
        "        self.update_buffer(self.env.reset())\n",
        "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
        "        return self.framebuffer\n",
        "    \n",
        "    def update_buffer(self, img):\n",
        "        offset = self.env.observation_space.shape[-1]\n",
        "        axis = -1\n",
        "        cropped_framebuffer = self.framebuffer[:,:,:-offset]\n",
        "        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)\n",
        "\n",
        "    def step(self, action):\n",
        "        new_img, reward, done, info = self.env.step(action)\n",
        "        self.update_buffer(new_img)\n",
        "        return self.framebuffer, reward, done, info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Init environment"
      ],
      "metadata": {
        "id": "Q6g94oBItKMl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "fVuXynjSvzie"
      },
      "source": [
        "\n",
        "def make_env():\n",
        "    env = gym.make(\"SpaceInvaders-v0\")\n",
        "    env = PreprocessAtari(env)\n",
        "    env = FrameBuffer(env, n_frames=4)\n",
        "    return env\n",
        "\n",
        "\n",
        "env = make_env()\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu8jQfGsvzim"
      },
      "source": [
        "### Rede"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sPSFBCyvzio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5de8675-e6fe-4705-a5ce-a1050fd50f35"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Conv2D, Dense, Flatten, InputLayer\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbNLkHuDvziq"
      },
      "source": [
        "from keras.layers import Conv2D, Dense, Flatten\n",
        "class DQNAgent:\n",
        "      def __init__(self, name, state_shape, n_actions, epsilon=0, reuse=False):\n",
        "        with tf.variable_scope(name, reuse=reuse): \n",
        "            self.network = keras.models.Sequential()\n",
        "    \n",
        "            self.network.add(Conv2D(32, (8, 8), strides=4, activation='relu',use_bias=False, input_shape=state_shape,kernel_initializer=tf.variance_scaling_initializer(scale=2)))\n",
        "            self.network.add(Conv2D(64, (4, 4), strides=2, activation='relu',use_bias=False,kernel_initializer=tf.variance_scaling_initializer(scale=2)))\n",
        "            self.network.add(Conv2D(64, (3, 3), strides=1, activation='relu',use_bias=False,kernel_initializer=tf.variance_scaling_initializer(scale=2)))\n",
        "            self.network.add(Conv2D(1024, (7, 7), strides=1, activation='relu',use_bias=False,kernel_initializer=tf.variance_scaling_initializer(scale=2)))\n",
        "            self.network.add(Flatten())\n",
        "            self.network.add(Dense(n_actions, activation='linear',kernel_initializer=tf.variance_scaling_initializer(scale=2)))\n",
        "            \n",
        "            \n",
        "            self.state_t = tf.placeholder('float32', [None,] + list(state_shape))\n",
        "            self.qvalues_t = self.get_symbolic_qvalues(self.state_t)\n",
        "            \n",
        "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "      def get_symbolic_qvalues(self, state_t):\n",
        "        \"\"\"Recebe observva챌찾o retorna Q values\"\"\"\n",
        "        qvalues = self.network(state_t)\n",
        "        \n",
        "        \n",
        "        assert tf.is_numeric_tensor(qvalues) and qvalues.shape.ndims == 2, \\\n",
        "            \"tensor 2d plz\" % repr(qvalues)\n",
        "        assert int(qvalues.shape[1]) == n_actions\n",
        "        \n",
        "        return qvalues\n",
        "  \n",
        "      def get_qvalues(self, state_t):\n",
        "        sess = tf.get_default_session()\n",
        "        return sess.run(self.qvalues_t, {self.state_t: state_t})\n",
        "    \n",
        "      def sample_actions(self, qvalues):\n",
        "        \"epislon-greedy a partir dos Q values\"\n",
        "        epsilon = self.epsilon\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "        best_actions = qvalues.argmax(axis=-1)\n",
        "        should_explore = np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon])\n",
        "        return np.where(should_explore, random_actions, best_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKZQAx98vzit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f887411-3c40-42be-96cf-e91e3498dfb3"
      },
      "source": [
        "agent = DQNAgent(\"dqn_agent\", state_dim, n_actions, epsilon=0.5)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0oplbmws0aQ"
      },
      "source": [
        "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
        "    rewards = []\n",
        "    s = env.reset()\n",
        "    for _ in range(n_games):\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            qvalues = agent.get_qvalues([s])\n",
        "            action =  agent.sample_actions(qvalues)[0]\n",
        "            s, r, done, _ = env.step(action)\n",
        "         \n",
        "            reward += r\n",
        "            if done: \n",
        "              s = env.reset()\n",
        "              break\n",
        "          \n",
        "        \n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcEMOVr0vzi6"
      },
      "source": [
        "### Experience replay\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdfJ8uo_w0T0"
      },
      "source": [
        "# C처digo da biblioteca baselines: https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
        "import random\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size):\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
        "        data = (obs_t, action, reward, obs_tp1, done)\n",
        "\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, idxes):\n",
        "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
        "        for i in idxes:\n",
        "            data = self._storage[i]\n",
        "            obs_t, action, reward, obs_tp1, done = data\n",
        "            obses_t.append(np.array(obs_t, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(reward)\n",
        "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
        "            dones.append(done)\n",
        "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
        "        return self._encode_sample(idxes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efn0J9MXvzi9"
      },
      "source": [
        "def play_and_record(agent, env, exp_replay, n_steps=1):\n",
        "    s = env.framebuffer\n",
        "    \n",
        "    reward = 0.0\n",
        "    for t in range(n_steps):\n",
        "        qvalues = agent.get_qvalues([s])\n",
        "        action = agent.sample_actions(qvalues)[0]\n",
        "        next_s, r, done, _ = env.step(action)\n",
        "        \n",
        "        exp_replay.add(s, action, r, next_s, done)\n",
        "        reward += r\n",
        "        if done:\n",
        "            s = env.reset()\n",
        "        else:\n",
        "            s = next_s\n",
        "    return reward\n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C28S0910vzjI"
      },
      "source": [
        "### Target networks\n",
        "\n",
        "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt3BJYRbvzjM"
      },
      "source": [
        "target_network = DQNAgent(\"target_network\", state_dim, n_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb9lUnZ0tAes"
      },
      "source": [
        "def load_weigths_into_target_network(agent, target_network):\n",
        "    assigns = []\n",
        "    for w_agent, w_target in zip(agent.weights, target_network.weights):\n",
        "        assigns.append(tf.assign(w_target, w_agent, validate_shape=True))\n",
        "    tf.get_default_session().run(assigns)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0IQNb3pvzjb"
      },
      "source": [
        "### Treino\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AbHB-60vzjc"
      },
      "source": [
        "obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim)\n",
        "actions_ph = tf.placeholder(tf.int32, shape=[None])\n",
        "rewards_ph = tf.placeholder(tf.float32, shape=[None])\n",
        "next_obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim)\n",
        "is_done_ph = tf.placeholder(tf.float32, shape=[None])\n",
        "\n",
        "is_not_done = 1 - is_done_ph\n",
        "gamma = 0.99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Ss1UJtvzjf"
      },
      "source": [
        "Take q-values for actions agent just took"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1qFprBwvzjg"
      },
      "source": [
        "current_qvalues = agent.get_symbolic_qvalues(obs_ph)\n",
        "current_action_qvalues = tf.reduce_sum(tf.one_hot(actions_ph, n_actions) * current_qvalues, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op08x3u0vzji"
      },
      "source": [
        "Q-learning TD error:\n",
        "\n",
        "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
        "\n",
        "Q-reference \n",
        "\n",
        "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiY2T2mYvzjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7660a487-173a-4854-b17a-afb50c230887"
      },
      "source": [
        "next_qvalues_target =  target_network.get_symbolic_qvalues(next_obs_ph)\n",
        "\n",
        "next_state_values_target = tf.reduce_max(next_qvalues_target, axis=-1)\n",
        "\n",
        "reference_qvalues = rewards_ph + gamma*next_state_values_target*is_not_done\n",
        "\n",
        "td_loss = tf.reduce_mean(tf.losses.huber_loss(labels=reference_qvalues, predictions=current_action_qvalues))\n",
        "\n",
        "optimizer=tf.train.AdamOptimizer(1e-5)\n",
        "train_step = optimizer.minimize(td_loss, var_list=agent.weights)\n",
        "\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xPdpbz3vzjs"
      },
      "source": [
        "### Main loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwvsqiQavzjw"
      },
      "source": [
        "exp_replay = ReplayBuffer(70000)\n",
        "play_and_record(agent, env, exp_replay, n_steps=10000)\n",
        "\n",
        "def sample_batch(exp_replay, batch_size):\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)\n",
        "    return {\n",
        "        obs_ph:obs_batch, actions_ph:act_batch, rewards_ph:reward_batch, \n",
        "        next_obs_ph:next_obs_batch, is_done_ph:is_done_batch\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGQPR9Dvvzjs"
      },
      "source": [
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import DataFrame\n",
        "moving_average = lambda x, span, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(span=span, **kw).mean().values\n",
        "%matplotlib inline\n",
        "\n",
        "mean_rw_history = []\n",
        "td_loss_history = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Wee4pdpKvzjz"
      },
      "source": [
        "agent.epsilon=1\n",
        "for i in trange(100000):\n",
        "    \n",
        "    play_and_record(agent, env, exp_replay, 10)\n",
        "    \n",
        "    _, loss_t = sess.run([train_step, td_loss], sample_batch(exp_replay, batch_size=64))\n",
        "    td_loss_history.append(loss_t)\n",
        "    \n",
        "    if i % 500 == 0:\n",
        "        load_weigths_into_target_network(agent, target_network)\n",
        "        agent.epsilon = max(agent.epsilon * 0.999, 0.01)\n",
        "\n",
        "    \n",
        "    if i % 1000 == 0:\n",
        "        mean_rw_history.append(evaluate(make_env(), agent, n_games=3))\n",
        "        \n",
        "    if i % 500 == 0:\n",
        "        clear_output(True)\n",
        "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
        "        plt.figure(figsize=[32,12])\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.title(\"mean reward per game\")\n",
        "        plt.plot(mean_rw_history)\n",
        "        plt.grid()\n",
        "\n",
        "        assert not np.isnan(loss_t)\n",
        "        plt.figure(figsize=[32, 12])\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.title(\"TD loss history (moving average)\")\n",
        "        plt.plot(moving_average(np.array(td_loss_history), span=100, min_periods=100))\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}